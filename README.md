# Subject Prediction with STW Filtering

This project focuses on running a subject prediction model using a filtered dataset based on the **STW (Standard Thesaurus for Economics)** subject taxonomy. The process includes preparing the dataset, filtering out irrelevant subjects, and experimenting with prompt-based and model-based approaches for subject classification.

---

## ğŸ“ Project Setup on KDSRV03

Follow these steps to run the project on the university server `KDSRV03`.

### ğŸ” Connect to Server

* If off-campus, connect via **FortiClient VPN**.
* SSH into the server using your university credentials ([connect via SSH](https://www.hiperf.rz.uni-kiel.de/caucluster/access/#user-account)).

### ğŸ“‚ Navigate to Project Directory

```bash
cd /data2/z2/stu213218/subject-prediction
```

### ğŸ Activate Virtual Environment

```bash
source modelenv/bin/activate
```

### ğŸ“¦ Install Dependencies

```bash
pip3 install -r requirements.txt
```

---

## ğŸ“Š Dataset Preparation

### 1. Check Dataset Content

Check whether abstracts contain any valid STW subject terms.

```bash
python3 src/subject_match.py
```

### 2. Filter STW-Only Subjects

Filter out data samples that contain only valid STW terms.

```bash
python3 filter_subject_matching.py
```

### 3. Reformat Validation Dataset

Convert the `val.json` format to a JSONL format (`val_fixed.json`) for evaluation:

```bash
python3 src/reformat_val_data.py
```

---

## ğŸ¤– Inference

Run inference on a sample abstract using the trained model:

```bash
python3 src/inference/inference.py
```

---

## ğŸ§ª Evaluation

Evaluate the model performance using precision, recall, F1 score, and hamming loss:

```bash
python3 src/evaluation/evaluate_model.py
```

Results are saved to:

```
data/processed/evaluation_results.json
```

---

## ğŸ“‚ File Structure Overview

* `data/processed/val.json` â€“ Raw validation input with abstract & labels.
* `data/processed/val_fixed.json` â€“ Reformatted NDJSON version for evaluation.
* `data/processed/trained_model/` â€“ Directory with tokenizer and model.
* `data/processed/label_encoder.pkl` â€“ MultiLabelBinarizer encoder used for label transformation.

---

## ğŸ” Git Workflow

```bash
git add .
git commit -m "Update README with full pipeline documentation"
git push
```

---

For further information or debugging, please check relevant scripts in:

* `src/inference/`
* `src/evaluation/`
* `src/data_preparation/`
